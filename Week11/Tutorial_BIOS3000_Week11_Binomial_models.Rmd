---
title: "Tutorial and assignment week 11 - Binomial models"
author: "Torbjørn Ergon and Tom Andersen"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: true
  pdf_document: default
header-includes:
  \usepackage{amsmath}
---

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This tutorial is a fairly comprehensive (but not in-depth) overview of binomial models commonly used in Biology. Sections (or parts of sections) marked with a "<span style="color:red">*</span>" are not part of the syllabus but are included to give you an overview of related topics that you likely have to read up on later in your career.

## Key terms and concepts covered in this tutorial

* Binomial models
* Maximum likelihood estimation
* Generalized linear models (glm)
* Contingency tables, odds and odds ratios
* Binomial glm with logit link
* Hazard rate and survival
* Binomial glm with complementary log-log link

## Prerequisites

We assume that you have a general understanding of terms and concepts from the previous tutorials and lectures, and in addition we assume you are familiar with the following (from STK1000 or similar):

* Binomial distribution
* Exponentials and logarithms (see the extra [Tutorial on Exponents and Logarithms](https://uio.instructure.com/files/635816/download?download_frd=1) at the end of the Week 1 module)

## Binomial models

In binomial models, we aim to predict probabilities. Instead of having a response variable that represents e.g. length, volume, mass, speed, light intensity, etc., the response variable is now a count of "successes" in a number of "trials". For example, we may want to investigate how the prevalence of a particular disease or another condition changes in a population over time or among locations. In a specific sample of $n$ individuals, we may find that $y$ individuals have the particular condition ($y$ is called "number of successes" even when we count something that is bad for the individuals (or other study units)). If the individuals have been sampled independently of each other, $y$ can be considered as having been drawn at random from a binomial distribution. One typically writes this as

$$
y \sim \text{Binomial}(n,p)
$$

You can read the "$\sim$" as "drawn from". This means that $P(Y=y) = \binom{n}{y} p^{y} (1-p)^{n-y}$, where $Y$ is the the stochastic variable representing the number of successes you may get in $n$ trials (the notation $P(Y=y)$ means "the probability that $Y$ takes the value $y$"). The parameter $p$ is the probability of success (e.g. testing positive for the disease) for a single study unit (e.g., individual) and study units are assumed to be independent. In the expression for the binomial distribution above, $\binom{n}{y} = n!/(y!(n-y)!)$. This is the "binomial coefficient" which is the number of different ways of distributing $y$ successes in a sequence of $n$ trials. In R, we can compute this with the function `choose(n,y)`.

Binomial models are different from regular linear models in that the response variable is not on the same scale as what we are trying to predict. What we are trying to predict is the parameter $p$ in the underlying binomial distribution. The probability $p$ is always the success probability of an *individual study unit* in the sample. Unlike e.g. the length of a body part, we cannot measure an individual's probability of e.g. getting a disease or dying (it either does or it doesn't). We say that such probabilities are "latent characteristics" which can only be predicted based on statistical modelling of data from many individuals (but the characteristics are nevertheless characteristics of the *individuals*).

---

<div class = "blue">
**FOR A DEEPER UNDERSTANDING (may be skipped):**

### <span style="color:red">*</span>Maximum likelihood estimation of $p$

Assume that we have a sample of 10 individuals and the true prevalence of a disease is 35%. In many such samples we should then on average get 3.5 individuals with the disease per sample. However, in a single sample we may get, say, 3 or 6 positive individuals. We may even get 10, but that is much less likely than getting 3 or 4 (we will certainly not get 3.5). We can use the expression for the binomial distribution above to calculate the probability of getting any number between 0 and 10 positive individuals in the sample. In R we can do this with the `dbinom()` function. Let's do that and make a plot:

```{r}
x = 0:10
Prob_x = dbinom(x, 10, 0.35)
plot(x, Prob_x, type="h")
```

If we had used the `rbinom()` function to sample a large number of values from this distribution this is the expected frequency of each value from 0 to 10 (you can confirm that these values sum to 1 with `sum(Prob_x)`).

Let's say we get $y=5$ positive individuals in our sample. Our best estimate of $p$ is then obviously $5/10 = 0.5$. We will now obtain this estimate through the maximum likelihood estimation method to see how it works. This method can be applied generally to a wide range of estimation problems, also when we cannot derive a mathematical expression for the estimate and have to use numerical computation.

According to the maximum likelihood principle we should pick the value of $p$ that maximizes the likelihood of getting the observed data. This value is the *maximum likelihood estimate* of $p$. To find this value, we can first plot the probability $Pr(Y=5)$ as a function of $p$:

```{r}
y = 5 # observed data
n = 10
L = function(p, y, n) choose(n,y) * p^y * (1-p)^(n-y) # see binomial probability expression above
p = seq(0, 1, by = 0.01) # values of p ranging from 0 to 1
plot(p, L(p, y, n)) # include argument type = "l" to get a line instead
```

(Instead of the `L` function we define here, we could have used the `dbinom` function we used above)

Note that this nice bell-shaped curve is not a probability distribution (the area under the curve is not 1). It is simply the probability of getting the data we have got for each hypothetical (candidate) value of $p$ plotted on the x-axis [Since the outcome $y$ is here a whole number, we can use the word "probability" instead of "likelihood"]. One generally writes this function as $L(\Theta; \text{Data})$ which can be read as "the likelihood of the parameters ($\Theta$) given the data". Note that the expression for $L(p; y, n)$ is the same as the expression for $Pr(Y=y)$, but instead of viewing the expression as a function of $y$ with known $p$, we view it as a function of the parameter $p$ with known $y$.

In this example we can find the maximum likelihood estimate of $p$ by picking out the value of the `p` vector where `L(p, y, n)` is at maximum:

```{r}
likelihood = L(p, y, n)
p[likelihood == max(likelihood)]
```

The plot above indicates that there is quite a bit of uncertainty in this estimate; if the true value of the parameter is 0.4 or 0.6, then we are almost as likely to get the observed data. Naturally, if we had more data, we would have got a narrower peak of the likelihood curve, indicating a more certain estimate.

Let's see what the likelihood looks like if we had observed 50 positive individuals in a sample of 100. Below, we add this as a blue line in the plot:

```{r}
plot(p, L(p, y=5, n=10), type="l", col="red") 
abline(v=y/n, col="red")
lines(p, L(p, y=50, n=100), col="blue") 
```

As expected, the peak of the blue curve is a lot narrower, indicating more certainty of the estimate. The fact that the likelihood values are much smaller when $n=100$ just reflects that it is much less likely to get exactly $y=50$ positives in a sample of 100 than it is to get $y=5$ in a sample of 10. All we care about here is the location of the peak (at $p=0.5$) and the width of the peak. We will later show that we can create a confidence interval for $\hat{p}$ based on the shape of the likelihood curve (or generally a multi-dimensional surface when we have more then one parameter in the model).

For several reasons one usually maximizes the log of the likelihood instead of the likelihood itself (what maximizes the log of the likelihood also maximizes the likelihood itself). We can also ignore constant terms (terms that do not include the parameter) because these do not affect the location of the peak. Hence, since $\log \left( p^y (1-p)^{(n-y)} \right) = y\log(p) + (n-y)\log(1-p)$ we maximize the log-likelihood function $\ell (p; y,n) = y\log(p) + (n-y)\log(1-p)$. Let us plot this function as well when $y=5$ and $n=10$:

```{r}
l = function(p, y=5, n=10) y*log(p) + (n-y)*log(1-p)
plot(p, l(p, y, n), type="l") 
abline(v=y/n, col="red")
```

Again, if we instead had observed 50 positives in a sample of 100, the curve would have been more pointy. Actually, we can use the negative inverse of the second derivative, evaluated at $\hat{p}$, as an estimate of the standard error of the estimate. You can explore interactively how the likelihood function changes with $n$ or $y$ in this [shiny app](http://shiny.stat.calpoly.edu/MLE_Binomial/) (select the "Log Likelihood Function" tab and play with changing the numbers of trials and successes).

In this case the likelihood function is so simple that you could have found the peak analytically by setting the first derivative of the function to zero and solving for $p$ (i.e., solve $\frac{\partial \ell (p)}{\partial p} = 0$; we leave this for you to show on your own). However, in complex likelihood functions with many parameters one needs to find the peak on a multidimensional surface, and it is often necessary to do this maximization by numerical computation (cannot be solved analytically). We will return to that later.

</div>

---

## Modelling $p$

We are often interested in trying to model the variation in the success probability ($p$) among individuals. For example, we may want to study how the probability of having a condition is associated with age or some environmental factor. Or perhaps we want to build a model to predict the risk of developing a heart disease such that people at risk can be warned or examined.

Let's say that the overall probability of being positive for a chronic medical disorder (e.g. diabetes) is 0.05. I.e., if we pick a random individual from the population, the probability that this individual has the condition is 0.05. We can call this the unconditional probability, because this is our prediction if we know nothing about the individual. If, however, we know the age of the individual, we may be able to give a better prediction (probability conditional on age). For example, a lower probability if the individual is young or a higher probability if the individual is old.

So, how can we make a model for an individual $i$'s probability of success, $p_i$? Just making $p_i$ a linear function of age would be a strange model as it is hard to come up with any biological theory for why we should expect a straight line. Besides such a model could predict probabilities outside the 0 to 1 range, which is nonsense. It is customary to transform the $p$-parameter to a log-odds scale before applying a linear model, such that $\log \left( \frac{p_i}{1-p_i} \right) = \mathbf{x}_i \mathbf{\beta}$ -- this is called a "logit" link function, which we will introduce below.

### Odds, odds ratios and the logit link

Odds is simply an alternate way of expressing probability, as in when we informally say "the chance is one in a million for that to happen" instead of saying "the probability is 0.000001". Formally, the odds of an event with probability $p$ is defined as $p / (1 - p)$, or the ratio of the probability of success to the probability of non-success. For example, if a betting agency states that the odds of a specific horse winning a race is $1/3$ ("one to three"), the probability that this horse will win is considered to be $p = 1/4 = 0.25$ (the probability of not winning is three times as high as the probability of winning; odds = $0.25/(1-0.25) = 1/3$). Note that the odds is just a sample transformation of the probability.

A consequence of this transformation is that we have changed the domain from the unit interval (0 to 1) to the positive part on the real line: the odds is close to zero when $p$ is low but grows towards $+\infty$ as $p$ approaches 1. If we log-transform the odds of a probability ($\log(p / (1-p)) = \text{logit}(p)$) we expand the domain to the whole real line. This means that $\text{logit}(p)$ approaches $-\infty$ when $p$ becomes very small, is zero when $p = 0.5$ (which is equivalent to $p / (1 - p) = 1$) and goes toward $+\infty$ as $p$ approaches 1. This also means that $\text{logit}(p)$ is better suited to be modelled by a weighted sum of the predictor variables, since these values can in principle take any real-numbered value (i.e., from $-\infty$ to $+\infty$).We can concretize this by looking at an example.

[The New York Times](https://www.nytimes.com/2021/02/07/world/south-africa-astrazeneca-vaccine.html) reported on the February 2nd 2021 from the AstraZeneca-Oxford trial in South Africa. In this [randomized controlled trial study](https://en.wikipedia.org/wiki/Randomized_controlled_trial), patients were randomly divided into two groups where one is injected with 2 doses of the AstraZeneca-Oxford vaccine while the other is given 2 doses of a placebo that looks identical to the medication. At the end of the study 19 of the 748 people in the group that was given the vaccine were infected with the new [B.1.351 variant of SARS-CoV-2 virus](https://en.wikipedia.org/wiki/Variants_of_SARS-CoV-2), compared with 20 of 714 people in the group that was given a placebo. This outcome can be tallied into a 2 by 2 *contingency table* representing the 4 possible outcomes with respect to B.1.351 infection for the two treatments.

```{r}
# https://www.nytimes.com/2021/02/07/world/south-africa-astrazeneca-vaccine.html

az <- matrix(c(19, 748-19, 20, 714-20), ncol=2)
rownames(az) <- c("Infected", "Not.Infect")
colnames(az) <- c("Vaccine", "Placebo")

as.table(az)
plot(as.table(az))
```

The key question in studies like this is how much of an effect the treatment has on the risk for infection. From the mosaic plot above we get a suspicion that the protective effect of the AstraZeneca-Oxford vaccine against B.1.351 infection is not large since the infected areas for Vaccine and Placebo look quite similar. We can start by calculating the probabilities of infection in the 2 treatment groups. Even if this may sound a bit morbid, we will consider "Infected" to mean success in this trial.

```{r}
(p.inf <- az["Infected", ] / colSums(az))
```

Notice how we use the rownames to extract different rows of the table. We see that there actually seems to be slightly higher probability of success (i.e., getting infected) from the placebo. We can also calculate the odds of success for the two treatments as 

```{r}
(odds.inf <- p.inf / (1 - p.inf))
```

We notice in passing that the probabilities for success and non-success in the same treatment have the same denominator (`colSums(az`)) such that we can also calculate the odds directly from the counts (`az["Infected", ] / ac["Not.Infect", ]`); check it out and take a moment to think about why this has to be so. Finally, we can calculate the odds ratio of the treatments, which represents the relative change in odds between the treatment and the placebo:

```{r}
(odds.ratio <- odds.inf["Vaccine"] / odds.inf["Placebo"])
```

The ratio is obviously close to 1 (i.e., identical odds in the two treatments), but we would also have liked to quantify the uncertainty in this estimate before we conclude anything about the efficacy of the AstraZeneca-Oxford vaccine against the new variant. We can approach this in many ways, but here we will show how to get this information by fitting the contingency table to a binomial generalized linear model (`glm`).

Binomial `glm`s are different from other linear model we have encountered in the sense that the dependent variable need to consist of two columns (the exception is *binary* binomial models where each observation is just a binary TRUE/FALSE variable); one for number of successes and one for number of failures. We usually do this by using the `cbind` function to make a two-column vector as the dependent variable. In our case the data are already in a 2 by 2 table, but we need to transpose this table to have one column for the number of successes (infected) and the number of failures (not infected). The predictor variable will just be a factor representing the treatments, which we extract from the column names of the contingency table. We also notice that `Placebo` will become the reference level by the default alphabetical ordering of factor levels. 

```{r}
(outcome <- cbind(Infected=az["Infected", ], Not.Infect=az["Not.Infect", ]))
(treatment <- factor(colnames(az)))
```

We then fit `y` to `x` using a `glm` of the binomial family with logit link (which is the default but which we specify here for clarity)

```{r}
summary(m <- glm(outcome ~ treatment, family=binomial(link="logit")))
```

By the default factor level ordering, the `(Intercept)` parameter is an estimate of log(odds) for infection in the placebo treatment: $\alpha =\log(odds)$ = `coef(m)[1]` = `r round(coef(m)[1], 3)`, which is equivalent to a probability $p = \exp(\alpha) /(1 + \exp(\alpha))$ = `exp(coef(m)[1]) / (1 + exp(coef(m)[1]))` = `r round(exp(coef(m)[1]) / (1 + exp(coef(m)[1])), 3)`, which is identical to the probability that we computed above, directly from the contingency table. The parameter `xVaccine` is the difference between the $\log(odds)$ for the placebo and the vaccine treatment. As we remember, a difference between logarithms is the same as the logarithm of the ratio of the arguments ($\log(A/B) = \log(A) - \log(B)$), such that this contrast can be interpreted as the logarithm of the *odds ratio* between the vaccine and placebo treatments:  $\beta =\log(odds ratio)$ = `coef(m)[2]` = `r round(coef(m)[2], 5)`. Anti-log of this coefficient $\exp(\beta)$ = `exp(coef(m)[2])` = `r round(exp(coef(m)[2]), 3)` thus represents the odds ratio between treatments, which is simply the odds ratio we calculate directly from the contingency table (`r round(odds.ratio, 3)`). 

The advantage of doing this estimation by a `glm` is that we can also get confidence intervals for the estimates; 

```{r}
exp(confint(m)[2, ])
```

In other words, we have 95% confidence that the odds of infection could be from 52% lower to 71% higher in the vaccine treatment relative to the placebo. Hence, we find little support for claiming that the AstraZeneca-Oxford vaccine substantially prevents infection by B.1.351 variant of the SARS-CoV-2 virus -- the vaccine may reduce the odds of infection by about 50%, but it is unlikely that the vaccine has a much stronger effect than that. If we get more data, the confidence interval will become narrower, but it will not necessarily be centered around 1 (equal odds).

### Other link functions

The logit (log-odds) link function we have used so far

$$
\log \left( \frac{p_i}{1-p_i} \right) = \mathbf{x}_i \mathbf{\beta}
$$

is the default link function when specifying `family = binomial` in the `glm` function, and is probably also the most used link function for binomial models. This is a choice based on tradition and convenience and not biological theory (it actually originates from gambling theory). When the probabilities we estimate represent something that does *not* accumulate over time, but rather something similar to an instantaneous coin flip, using the logit link, and hence interpreting contrasts as odds-ratios, is a good choice. However, when we model probabilities for something occurring or not occurring in a defined period of time, it makes much more sense to us the 'loglog link' (when estimating the probability of something *not* occurring in an interval),

$$
\log \left( -\log \left( p \right) \right) = \mathbf{x}_i \mathbf{\beta}
$$

or the 'complementary loglog link' (when estimating the probability of something occurring at least once in an interval)

$$
\log \left( -\log \left( 1-p \right) \right) = \mathbf{x}_i \mathbf{\beta}
$$

The latter is often called the 'cloglog' link.

For example, if we want to estimate the probability that individuals survive an interval of a fixed length (the longer the interval, the lower the probability of survival), it would make sense to use the 'loglog' link. If we instead model the probability of mortality, we would use the 'cloglog' link.

With these link functions, contrasts in the linear predictors become the log of hazard ratios -- i.e., the antilog of a contrast is the ratio of two hazard rates, where a hazard rate is the "instantaneous failure rate" (defined more exactly below). We don't prioritize time to work with these link-functions in this course, but this is something you should read up on if you later need to model survival or mortality probabilities (or anything occurring or not occurring within time intervals). We provide some more details for those interested in the box below.

---

<div class = "blue">
**ADVANCED (may be skipped):**

### <span style="color:red">*</span>Hazard rate and survival

Maa skrives om litt

Although logit is the default link function for binomial `glm`s in R, we should be aware that this function is mainly a choice based on tradition and convenience and that it is not derived from any biological theory (it actually originates from gambling). Let's instead start with considering the per-time rate at which the disorder occurs per susceptible individual in the population. This type of rate is called a *hazard rate*, and we will denote it as $h$. If the hazard rate stays constant throughout the life of an individual, the probability that an individual will have avoided developing the disorder until age $A$ is $\exp(-hA)$, and the probability that the individual has developed it is $p = 1-\exp(-hA)$. The hazard rate $h$ is a ratio-scaled metric (see the tutorial of Week 5) and is natural to model with a log-linear model (i.e., modelling multiplicative effects on $h$). We can also easily expand the model to $p = 1-\exp(-hA^b)$. If $b>1$, the risk of developing the disorder increases with age, if $b<1$, the risk decreases with age, but older individuals will always have a higher probability of having developed the disorder as they have been exposed to the risk for a longer time. This model corresponds to a *accelerated failure time model* in survival analysis where the "failure times" (the ages at which the disorder develops in our case) is observed, but in our case we just observe whether the disorder has developed or not at different ages. With this model, the age at which the disorder develops will be [Weibull distributed](https://en.wikipedia.org/wiki/Weibull_distribution). 

Note that $\log(hA^b)=\log(h) + b\log(A)$. Hence, our model can be written on the form $p = 1-\exp(-\exp(\eta))$ where $\eta=\beta_0+\beta_1x_1$, $\beta_0=\log(h)$, $\beta_1=b$ and $x_1=\log(A)$. This corresponds to $\log(-\log(1-p))=\eta = \mathbf{x}_i \mathbf{\beta}$ which is called the "complementary loglog" link function ("cloglog" for short). This is one of the standard link functions available in the `glm()` function for "generalized linear models" in R. Hence, we can fit our model by using a standard "cloglog" link and include log of age as one of the predictor variables.

Let us simulate some data and fit the model in `glm()` to check that we got things right. First, we simulate some data that we plot:

```{r}
n = 100 # Number of individuals
Age = runif(n, 20, 90) # Draw ages from a uniform distribution (not realistic, but works for the purpose)
h = 1/100 # If b = 1, time until the disorder develops is exponentially distributed with mean 1/h
b = 1.1 # To see the relationship between accelerated Age and Age, use plot(Age, Age^b); abline(0,1)
eta = log(h) + b*log(Age)
p = 1-exp(-exp(eta))
y = rbinom(n, 1, p)
plot(Age, y)
points(Age, p, col="blue")
```

The blue points are here the individual probabilities of having developed the disorder while the black points are the realized data (0 = "no disorder", 1 = "disorder").

Now we can fit these data to the model and see if we get parameter estimates close to the true values:

```{r}
fit = glm(y ~ log(Age), family = binomial(link = "cloglog"))
summary(fit)
```

The intercept is here $\log(h)$ where the true value is $\log(1/100) =$ `r round(log(1/100), 2)`, and the true slope with `log(Age)` is `r b`. With this low sample size there will be some difference between the point estimates and the true values, but the confidence interval based on $\pm 2$SE should cover the true values about 95% of the time. If you increase the sample size to 100,000 you will see that the point estimates become very close to the true values of the parameters.

</div>

---

## <span style="color:red">*</span>Model fitting by maximum likelihood from scratch 

If you've had enough of maximum likelihood estimation for now, you can skip ahead to the exercises. If you want to know what goes on under the hood in the `glm()`, read on!

Above, we derived the binomial log-likelihood function when $p$ is the same for all individuals in the sample. This log-likelihood also applies when $n=1$. How do we go about constructing a likelihood function when each individual has its own $p_i$ and $y_i$?

Remember that the probability of a series of **independent** events is the product of the probability of each event, $\Pr(A \cap B \cap C) = \Pr(A) \Pr(B) \Pr(C)$ (the assumption of independence is essential here). This means that the likelihood for the data on all individuals is the product of the likelihood for each individual. In the log-likelihood, the product becomes a sum (remember that $\log(xy) = \log(x) + \log(y)$). Hence, the log-likelihood for the data is

$$
\ell (\mathbf{p} ; \mathbf{y}) = \sum_i^{N} \left( y_i\log(p_i) + (1-y_i)\log(1-p_i) \right)
$$

Note that we have here replaced $n$ with $1$ for all individuals and $N$ is the number of individuals.

It is quite straightforward to modify the code for simulating data that we used above to become a log-likelihood function for the particular model. We basically just need to replace the random drawing of individual values with a calculation of the likelihood. We do this below while calculating the linear predictor in the general way as $\eta = \mathbf{X} \mathbf{\beta}$ (see last tutorial). We also compute the negative of the log-likelihood function because optimization routines in R (and elsewhere) do minimization and not maximization. 

```{r}
inv_cloglog = function(x) 1-exp(-exp(x)) # The inverse cloglog link function
nll_cloglog = function(beta, y, X, inv_link = inv_cloglog){
  eta = X %*% beta
  p = inv_link(eta)
  nll = -sum(y*log(p) + (1-y)*log(1-p))
}

```

This is the general negative log-likelihood function for a binomial model with a cloglog link where the arguments `beta` is the parameter vector, `y` is the observed binary response and `X` is the design matrix for cloglog(p). We can find the values in the parameter vector that minimize this function by using the `optim()` function in R. We do this below with the specific model we formulated above for our simulated data:

```{r}
Sim_data = data.frame(y=y, Age=Age)
X = model.matrix(~log(Age), Sim_data)
beta_start = c(0,0) # starting values for the numerical optimization (starting at 0 usually works well for glm's)
(fit = optim(beta_start, fn=nll_cloglog, y=Sim_data$y, X=X, hessian = TRUE))
```

Here, `$par` holds the parameter estimates (you can confirm that these are the same as what we got from `glm()`) and `$value` is the negative log likelihood at minimum. The next three outputs is a summary of the optimization process; as long as `$convergence` is 0 (indicating convergence) and there are no messages, we as happy. The last output, `$hessian`, is given to us because we asked for it in the call to `optim()` by specifying `hessian = TRUE`. The hessian matrix is a matrix of second order partial derivatives of the negative log likelihood with respect to the parameters, evaluated at the optimum. Specifically, the element in row $i$ and column $j$ equals $\frac{\partial^2 \ell}{\partial \beta_i \partial \beta_j}$. This matrix is very useful because the inverse of the hessian is an estimate of the variance-covariance matrix of the parameter estimators. The square-root of the diagonal of this is the standard errors of the parameter estimates:

```{r}
vc = solve(fit$hessian)
(SE = sqrt(diag(vc)))
```

Again, we can confirm that these values are practically the same as the standard errors returned by `glm()`.

Let's wrap up these procedures in our own glm function:

```{r}
my_glm = function(formula, data, y, start=NULL, inv_link = inv_cloglog){
  X = model.matrix(formula, data = data)
  if(is.null(start)) start = rep(0, ncol(X)) # Setting starting values unless supplied
  names(start) = dimnames(X)[[2]]
  fit = optim(start, fn=nll_cloglog, y=Sim_data$y, X=X, inv_link=inv_link, hessian = TRUE)
  vcov = solve(fit$hessian)
  return(
    list(
      optim_output = fit,
      par = fit$par,
      se = sqrt(diag(vc)),
      vcov = vcov,
      inv_link = inv_cloglog,
      formula = formula
    )
  )
}
fit_my_glm = my_glm(~log(Age), Sim_data, Sim_data$y)
```

The output is not shown here, but you can type `fit_my_glm` in the console window to see it.

We have now made our own function for fitting a generalized linear model in R! This function doesn't (yet) do anything that we cannot do with `glm()`. However, we can easily modify it to e.g. include non-linear components (e.g., we could include several additive (rather than multiplicative) components of the disorder hazard rate).

For completeness, we can also make a function for computing predictions from the model fit:

```{r}
predict_my_glm = function(fit, newdata){
    X = model.matrix(fit$formula, data = newdata)
    eta = X %*% fit$par
    vc = X %*% fit$vcov %*% t(X)
    se = sqrt(diag(vc))
    pred = data.frame(
      p_hat = fit$inv_link(eta),
      lwr = fit$inv_link(eta - 2*se),
      upr = fit$inv_link(eta + 2*se)
    )
    return(cbind(newdata, pred))
}
```

We can use this for plotting the predictions with 95% confidence interval:

```{r}
newdata = data.frame(Age = 20:90)
pred = predict_my_glm(fit_my_glm, newdata)
plot(pred$Age, pred$p_hat, type="l", ylim=c(0,1), xlab="Age", ylab="Probability of disorder")
lines(pred$Age, pred$lwr, lty=2)
lines(pred$Age, pred$upr, lty=2)
```

## <span style="color:red">*</span>Most Probable Number (MPN)

The Most Probable Number (MPN) method has been used in microbiology since at least the 1930ies to quantify the number of organisms in a sample. The method is particularly useful for quantifying organisms which cannot be identified by morphology but rather by their ability to grow in specific media, which was the case for much of bacterial systematics before emergence of current DNA-based classifications. The MPN method has, for example, remained a standard method for estimating the number of fecal coliform bacteria in water samples. Fecal coliforms, which live exclusively in the guts of endothermic animals, are considered good indicators of fresh sewage pollution, and thus an important hygienic quality parameter for assessing the suitability of a water source for consumption and recreation.

The basic principle for the MPN method is to make a dilution series of the original sample and record whether growth can be detected or not at the different dilution levels. Dilution levels are typically chosen as a geometric series on powers of 2 or 10. One may for example add 1, 1/2, 1/4, 1/8, 1/16, etc millilitres (mL) of the sample to tubes containing a volume of sterile growth medium that is selective for fecal coliforms. The tubes are then incubated at optimal growth conditions (37º C for fecal coliforms) and growth recorded as visible turbidity in a tube. The different dilution levels are typically replicated several times to increase the precision of the method.

The statistical basis for the MPN method was worked out by R. A. Fisher himself and used by him as an example of maximum likelihood estimation. We will here use simulation to illustrate Fisher’s analysis of the MPN method, and show how the MPN estimate can be found by a particular kind of generalized linear model.

Assume that we have a sample that contains an average of $\lambda$ bacteria per mL of liquid. Thus the average number of bacteria in 1/2 mL, 1/4 mL, 1/8 mL, ... will be $\lambda/2$, $\lambda/4$ $\lambda/8$, ... and generally $\lambda 2^{-i}$ in $2^{-i}$ mL of liquid. If the bacteria are randomly distributed in the original sample, then the number of bacteria ($N$) in a population of 1 mL subsamples from the same source can be described by a Poisson distribution with intensity parameter $\lambda$ ($N \sim \text{Poisson}(\lambda)$), while the distribution of bacterial numbers in a population of $2^{-i}$ mL subsamples will be $\sim \text{Poisson}(\lambda 2^{-i})$.

A certain subsample will only give growth if it contained at least 1 bacterial cell, which means that $P(growth) = P(N>0)$. Since the domain of $N$ is the natural numbers (0, 1, 2, ... then ) then the law of complementary probability tells us that $P(N>0) = 1 - P(N=0)$. Since Poisson probability for $N=0$ is simply $exp(-\lambda)$ (check this out yourself!), the probability of growth in dilution $i$ becomes $p_i = 1 - exp(-\lambda 2^{-i})$. By rearranging the terms and log-transforming both sides twice (check out yourself!) we can show that this equation can be rewritten as

$$log(-log(1 - p_i)) = log(\lambda) - log(2) i$$
In other words, the complementary log-log of the probability of growth should be a linear function of the dilution level $i$ with slope $-log(2)$ and intercept equal to the logarithm of the initial bacterial abundance ($\lambda$), which is what we sought to estimate.

We can simulate the MPN method by using the `rpois()` function for generating random, Poisson-distributed numbers. We assume that the true number of bacteria, $\lambda$ = 2 / mL, and that we use 7 dilution levels (1/1, 1/2, 1/4 ... 1/64) and 8 replicates at each dilution level.

```{r}
Lambda <- 2
i.k <- rep(0:6, rep(8, 6+1))
lambda.k <- Lambda * (2^(-i.k))

set.seed(123456) # Just so we get the same numbers every time...
N.k <- rpois(length(i.k), lambda = lambda.k)
```

This gives us 7 * 8 = 56 integers in 7 dilution level groups. We can use the `aggregate()` function to count the number of non-zeros (=growth) in each  the dilution groups, and make a matrix that shows number of growths for the different dilution levels:

```{r}
growth <- aggregate(N.k > 0, by = list(i.k), sum)$x
no.growth <- (8 - growth)

i <- 0:6
rbind(i, growth, no.growth)
```

This means that we can see the MPN method as a set of binomial experiments where each group of 8 replicates has the same probability of growth, and where the complementary log-log of each growth probability is linearly related to the dilution level (i). In other words, we should be able to describe the MPN experiment by a generalized linear model with the default link function of the binomial family (the logit link) replaced by a complementary log-log link. 

```{r}
summary(m <- glm(cbind(growth, no.growth) ~ i, family = binomial(link = "cloglog")))
```

The `(Intercept)` is an estimate for $log(\lambda)$, such that its anti-log becomes an estimate of to original bacterial abundance `exp(coef(m)[1]` = `r round(exp(coef(m)[1]), 3)` which in this particular simulation is slightly above the true value (2). Slope is fairly close to the theoretical value $-log(2)$ = `r round(-log(2), 3)`.

We can also compute 95% confidence intervals for the estimates and back-transform:

```{r}
exp(confint(m))
```

From which we see that both confidence intervals enclose the true values: 2 for the intercept and $exp(-log(2))$ = 1/2 for the slope.

## Exercise: Side-effects in AstraZeneca vaccinations

There has recently been raised concerns about possible serious side-effects of the AstraZeneca vaccine against covid-19 disease. Occurrence of sometimes fatal blood clots in otherwise healthy individuals injected with the AstraZeneca vaccine have lead many countries to pause the use of it, especially for younger individuals.According to an article in Norwegian newspaper [Aftenposten](https://www.aftenposten.no/norge/i/yRxgga/legemiddelverket-lite-realistisk-at-vi-kan-plukke-ut-hvem-som-ikke-boe) on March 23rd., 2021, 30 cases of serious blood clot complications have been reported among the 20 million who have been injected with this vaccine worldwide, while this condition has developed in 6 cases among the 130 000 who have received this vaccine in Norway. 

Use the 4 numbers given in the article to compute the side-effect probabilities in Norway and worldwide, and the odds ratio for developing side-effects between the two groups. Estimate the same odds ratio with a generalized linear model of the binomial family, and use this model to construct a 95% confidence interval for the odds ratio. Is this result in support of the odds being different between Norway and the world in general? If so, what could be the cause? Discuss possible explanations including:

* Different age groups vaccinated (mainly health-care workers in Norway)?
* Differences between populations (genetics, nutrition, life style, etc.)?
* A bad vaccine batch shipped to Norway?
* Differences in detection / reporting of blood clots?


## Assignment: Diabetes in the Pima Indians

The Akimel O'odham or Pima are a group of Native Americans living in an area consisting of what is now central and southern Arizona (USA) and Sonora (Mexico). The name "Pima" apparently comes from a phrase that means "I don't know", used repeatedly in their initial meeting with Europeans.

![](Pima map.png)

As with other Native Americans, the Pima people have a higher prevalence of type 2 diabetes than is observed in Caucasian populations. While they do not have a greater risk than other tribes, the Pima people have been the subject of intensive study of diabetes, in part because they form a homogeneous group. The general increased diabetes prevalence among Native Americans has been hypothesized as the result of the interaction of genetic predisposition (the thrifty phenotype or thrifty genotype as suggested by anthropologist Robert Ferrell in 1984) and a sudden shift in diet from traditional agricultural goods towards processed foods in the past century. 

The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The following variables were recorded: 

*  `npreg`: Number of times pregnant
*  `glu`: Oral glucose tolerance test 
*  `bp`: Diastolic blood pressure (mm Hg)
*  `skin`: Triceps skin fold thickness (mm)
*  `bmi`: Body mass index (kg / m^2^)
*  `ped`: Diabetes pedigree function
*  `age`: Age (years)
*  `diabetes`: Diabetes diagnosed ("No" or "Yes")

The original data set contained numerous missing values which apparently were encoded as zeros, such that there were a number of cases with e.g. blood pressure = 0! The file "Pima-indian-diabetes.csv" contains only the 332 complete cases in the original data set (i.e., no missing values). The Diabetes Pedigree Function (`ped`) was developed by [Smith et al. (1988)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/pdf/procascamc00018-0276.pdf) to provide a synthesis of the diabetes history in relatives and the genetic relationship of those relatives to the subject.It uses information from parents, grandparents, siblings, aunts and uncles, and first cousins and provides a measure of the expected genetic influence of affected and unaffected relatives on the subject's eventual diabetes risk. The oral glucose tolerance test is a standard test for diabetes involving measuring plasma glucose 2 hours after oral intake of an amount of glucose. The triceps skin fold test is a standard measure of subcutaneous fat, and as such, an indicator of obesity similar to the body mass index. The other variables are largely self-explanatory. The last variable is a binary indicator for whether the person has been diagnosed with diabetes or not.

A. Read the file "Pima-indian-diabetes.csv" into R. For the rest of the exercise it is more convenient if we change the variable `diabetes` from factor to a logical variable which is `TRUE` for factor level "Yes" (hint: `diabetes == "Yes"` gives a logical variable; notice the double equals sign). Use `head()` and `summary()` to assure yourself the file has been read properly.

B. Our task is to predict `diabetes` from other variables. It is therefore a good idea to first look at relationships between the predictor variables by making scatterplot and correlation matrices (hint: `plot()` on a `data.frame` object is identical to using the `pairs()` function). *Are there any predictor variables that are closely related? If so, can you think of explanations for this?* (hint: look at for example `bmi` vs. `skin`, or `age` vs. `npreg`). *Which considerations should we make when using any of these pairs in a model?*

C. Use boxplots to investigate the distributions of at least 4 predictor variables grouped by diabetes status. You can use the `boxplot()` function with a formula like `glu ~ factor(diabetes)`. Notice that this works better if you cast the now logical variable `diabetes` into a factor again. Using the argument `horizontal=TRUE` in the `boxplot()` call makes the figure easier to read.

D. Pick one of the predictor variables you investigated in part C and make a univariate binomial `glm` with logit link where you use this variable to predict diabetes status. Notice that since our dependent variable is binary, we do not need to use `cbind()` to make a 2-column vector for the dependent variable. *Explain how you interpret the intercept and slope estimates in this model.* *Make a 95% confidence interval for the relative change in odds per unit change in the predictor variable* (depending on the variable you choose, it may increase readability to compute the relative change per 10 or 100 units).

E. Use the `predict()` function on your model. If your model is called `m` then `predict(m)` gives model predictions for all observations in link function units (i.e., logit units), while `predict(m, type="response")` gives predictions in probability units. Make a new variable `diabetes.true` equal to the `diabetes` column in your data frame, and a variable `diabetes.pred = (p > 0.5)` where `p` is a variable  containing the predicted probabilities from your model. Make a contingency table for true and predicted diabetes status (`table(diabetes.true, diabetes.pred`). *How do you interpret the diagonal and off-diagonal elements in this matrix? How many false positives and false negatives does your model predict?* Compute the Positive Prediction Value (PPV) for your model as the number of true positives divided by total positive predictions. *How useful would you consider your model to be for the predicting risk of diabetes in Pima Indians?*

F. Make a visualization of your model. A line drawn through the predicted probabilities looks messy because the observations are not necessarily ordered by the predictor variable of your model. We can make predictions for arbitrary values of the predictor variable by using the `newdata=` argument in the `predict()` function. `newdata` must be assigned to a list or data frame that includes exactly the same variable names as in the model. So, if we use `age` as predictor variable, and we are interested in predicting diabetes status over the age interval spanned by the data, we can set `newdata=data.frame(age = seq(20, 80))`. If we set `se.fit=TRUE` then `predict` will return a 2-column vector with predicted log(odds) and their standard errors. Use this information to produce a plot of predicted probability of diabetes with a 95% confidence belt for the predictions. Notice that you need to make predictions on link-function scale (`type="link`) and back-transform the confidence limits with the inverse link function, as illustrated in the `predict_my_glm` function above. Figure also out how to use the `rug()` function to represent observations of the predictor variable along the x-axis.

